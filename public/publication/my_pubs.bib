
@article{cushingNeurodynamicsConnectivityFacial2018,
	title = {Neurodynamics and connectivity during facial fear perception: {The} role of threat exposure and signal congruity},
	volume = {8},
	issn = {2045-2322},
	shorttitle = {Neurodynamics and connectivity during facial fear perception},
	url = {http://www.nature.com/articles/s41598-018-20509-8},
	doi = {10.1038/s41598-018-20509-8},
	language = {en},
	number = {1},
	urldate = {2018-12-19},
	journal = {Scientific Reports},
	author = {Cushing, Cody A. and Im, Hee Yeon and Adams, Reginald B. and Ward, Noreen and Albohn, Daniel N. and Steiner, Troy G. and Kveraga, Kestutis},
	month = dec,
	year = {2018},
	file = {Cushing et al. - 2018 - Neurodynamics and connectivity during facial fear .pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/2Q3PT5TC/Cushing et al. - 2018 - Neurodynamics and connectivity during facial fear .pdf:application/pdf},
}

@incollection{albohnSocialVisionIntersection2016,
	title = {Social {Vision}: {At} the {Intersection} of {Vision} and {Person} {Perception}},
	isbn = {978-0-12-800935-2},
	url = {https://linkinghub.elsevier.com/retrieve/pii/C20130186230},
	language = {en},
	urldate = {2018-12-19},
	booktitle = {Neuroimaging {Personality}, {Social} {Cognition}, and {Character}},
	publisher = {Elsevier},
	author = {Albohn, Daniel N. and Adams, Reginald B.},
	editor = {Absher, JR and Cloutier, J},
	year = {2016},
	doi = {10.1016/C2013-0-18623-0},
	file = {2016 - Neuroimaging Personality, Social Cognition, and Ch.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/LHJULDEX/2016 - Neuroimaging Personality, Social Cognition, and Ch.pdf:application/pdf},
}

@article{matteucci2012ExerciseBehaviorColiege,
	title = {Exercise {Behavior} {Among} {Coliege} {Students} and {Sex} {Differences} in a {Health}-{Promotive} {Intervention}},
	abstract = {This study examined the effectiveness of a university-based Life Fitness course on college students' health behavior in terms ofthe number of hours students spent doing various types of exercise-related activities (moderate activities, hard activities, and very hard activities) before and after the course. Participants were asked to complete a series of questions regarding daily activity levels and habits both before and after the completion ofthe course. Results revealed significant increases in the mean number of hours spent on each type of exercise-related activity. Specifically, for moderate activities, there were significant main effects for time, F\{\vphantom{\}}1, 187) = 6.70, p = .01, n 2 = .04, and sex, F\{\vphantom{\}}1, 187) = 18.80, p {\textless} .001, ri {\textasciicircum} = .09, with increases in these activities across time and men reporting higher mean levels of this activity compared to women. For hard activities, there was a significant time xsex interaction, F\{\vphantom{\}}1, 112) = 5.90, p = .03, r) {\textasciicircum} = .04, indicating more dramatic increases for men during this period. For very hard activities, there was a significant main effect for sex, i{\textasciicircum}(l, 112) = 11.40, /? {\textless} .001, r) {\textasciicircum} = .09, indicating that men reported higher mean levels ofthese activities relative to women. Findings yield important implications for future research on the relationship between health-promotive intervention and students' health-related behaviors and the establishment of healthy attitudes and behaviors that persist into adulthood.},
	language = {en},
	journal = {Psi Chi Journal of Reserach},
	author = {Matteucci, Alyssa J and Albohn, Daniel N. and Stoppa, Tara M. and Mercier, Wendy},
	year = {2012},
	pages = {9},
	file = {Matteucci et al. - 2012 - Exercise Behavior Among Coliege Students and Sex D.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/7BRTZYWB/Matteucci et al. - 2012 - Exercise Behavior Among Coliege Students and Sex D.pdf:application/pdf},
}

@article{imDifferentialHemisphericVisual2017,
	title = {Differential hemispheric and visual stream contributions to ensemble coding of crowd emotion},
	volume = {1},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-017-0225-z},
	doi = {10.1038/s41562-017-0225-z},
	language = {en},
	number = {11},
	urldate = {2018-12-19},
	journal = {Nature Human Behaviour},
	author = {Im, Hee Yeon and Albohn, Daniel N. and Steiner, Troy G. and Cushing, Cody A. and Adams, Reginald B. and Kveraga, Kestutis},
	month = nov,
	year = {2017},
	pages = {828--842},
	file = {Im et al. - 2017 - Differential hemispheric and visual stream contrib.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/23E8KGXY/Im et al. - 2017 - Differential hemispheric and visual stream contrib.pdf:application/pdf},
}

@article{wagenmakersRegisteredReplicationReport2016,
	title = {Registered {Replication} {Report}: {Strack}, {Martin}, \& {Stepper} (1988)},
	volume = {11},
	issn = {1745-6916, 1745-6924},
	shorttitle = {Registered {Replication} {Report}},
	url = {http://journals.sagepub.com/doi/10.1177/1745691616674458},
	doi = {10.1177/1745691616674458},
	abstract = {According to the facial feedback hypothesis, people’s affective responses can be influenced by their own facial expression (e.g., smiling, pouting), even when their expression did not result from their emotional experiences. For example, Strack, Martin, and Stepper (1988) instructed participants to rate the funniness of cartoons using a pen that they held in their mouth. In line with the facial feedback hypothesis, when participants held the pen with their teeth (inducing a “smile”), they rated the cartoons as funnier than when they held the pen with their lips (inducing a “pout”). This seminal study of the facial feedback hypothesis has not been replicated directly. This Registered Replication Report describes the results of 17 independent direct replications of Study 1 from Strack et al. (1988), all of which followed the same vetted protocol. A meta-analysis of these studies examined the difference in funniness ratings between the “smile” and “pout” conditions. The original Strack et al. (1988) study reported a rating difference of 0.82 units on a 10-point Likert scale. Our meta-analysis revealed a rating difference of 0.03 units with a 95\% confidence interval ranging from −0.11 to 0.16.},
	language = {en},
	number = {6},
	urldate = {2018-12-19},
	journal = {Perspectives on Psychological Science},
	author = {Wagenmakers, E.-J. and Beek, T. and Dijkhoff, L. and Gronau, Q. F. and Acosta, A. and Adams, R. B. and Albohn, D. N. and Allard, E. S. and Benning, S. D. and Blouin-Hudon, E.-M. and Bulnes, L. C. and Caldwell, T. L. and Calin-Jageman, R. J. and Capaldi, C. A. and Carfagno, N. S. and Chasten, K. T. and Cleeremans, A. and Connell, L. and DeCicco, J. M. and Dijkstra, K. and Fischer, A. H. and Foroni, F. and Hess, U. and Holmes, K. J. and Jones, J. L. H. and Klein, O. and Koch, C. and Korb, S. and Lewinski, P. and Liao, J. D. and Lund, S. and Lupianez, J. and Lynott, D. and Nance, C. N. and Oosterwijk, S. and Ozdoğru, A. A. and Pacheco-Unguetti, A. P. and Pearson, B. and Powis, C. and Riding, S. and Roberts, T.-A. and Rumiati, R. I. and Senden, M. and Shea-Shumsky, N. B. and Sobocko, K. and Soto, J. A. and Steiner, T. G. and Talarico, J. M. and van Allen, Z. M. and Vandekerckhove, M. and Wainwright, B. and Wayand, J. F. and Zeelenberg, R. and Zetzer, E. E. and Zwaan, R. A.},
	month = nov,
	year = {2016},
	pages = {917--928},
	file = {Wagenmakers et al. - 2016 - Registered Replication Report Strack, Martin, & S.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/Y4P8I59T/Wagenmakers et al. - 2016 - Registered Replication Report Strack, Martin, & S.pdf:application/pdf},
}

@article{adamsSocialVisionApplying2017,
	title = {Social {Vision}: {Applying} a {Social}-{Functional} {Approach} to {Face} and {Expression} {Perception}},
	volume = {26},
	issn = {0963-7214, 1467-8721},
	shorttitle = {Social {Vision}},
	url = {http://journals.sagepub.com/doi/10.1177/0963721417706392},
	doi = {10.1177/0963721417706392},
	abstract = {A social-functional approach to face processing comes with a number of assumptions. First, given that humans possess limited cognitive resources, it assumes that we naturally allocate attention to processing and integrating the most adaptively relevant social cues. Second, from these cues, we make behavioral forecasts about others in order to respond in an efficient and adaptive manner. This assumption aligns with broader ecological accounts of vision that highlight a direct action-perception link, even for nonsocial vision. Third, humans are naturally predisposed to process faces in this functionally adaptive manner. This latter contention is implied by our attraction to dynamic aspects of the face, including looking behavior and facial expressions, from which we tend to overgeneralize inferences, even when forming impressions of stable traits. The functional approach helps to address how and why observers are able to integrate functionally related compound social cues in a manner that is ecologically relevant and thus adaptive.},
	language = {en},
	number = {3},
	urldate = {2018-12-19},
	journal = {Current Directions in Psychological Science},
	author = {Adams, Reginald B. and Albohn, Daniel N. and Kveraga, Kestutis},
	month = jun,
	year = {2017},
	pages = {243--248},
	file = {Adams et al. - 2017 - Social Vision Applying a Social-Functional Approa.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/J2G32VAF/Adams et al. - 2017 - Social Vision Applying a Social-Functional Approa.pdf:application/pdf},
}

@incollection{garridoAutomaticity2018,
	address = {Cham},
	title = {Automaticity},
	isbn = {978-3-319-28099-8},
	url = {http://link.springer.com/10.1007/978-3-319-28099-8_1781-1},
	language = {en},
	urldate = {2018-12-19},
	booktitle = {Encyclopedia of {Personality} and {Individual} {Differences}},
	publisher = {Springer International Publishing},
	author = {Garrido, Carlos O. and Albohn, Daniel N.},
	editor = {Zeigler-Hill, Virgil and Shackelford, Todd K.},
	year = {2018},
	doi = {10.1007/978-3-319-28099-8_1781-1},
	pages = {1--4},
	file = {Garrido and Albohn - 2018 - Automaticity.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/HHC9QRN8/Garrido and Albohn - 2018 - Automaticity.pdf:application/pdf},
}

@article{freudenbergEmotionalStereotypesTrial2019,
	title = {Emotional stereotypes on trial: {Implicit} emotion associations for young and old adults.},
	issn = {1931-1516, 1528-3542},
	shorttitle = {Emotional stereotypes on trial},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/emo0000626},
	doi = {10.1037/emo0000626},
	abstract = {Individuals use naïve emotion theories, including stereotypical information on the emotional disposition of an interaction partner, to form social impressions. In view of an aging population in Western societies, beliefs on emotion and age become more and more relevant. Across 10 studies, we thus present findings on how individuals associate specific affective states with young and old adults using the emotion implicit association test. The results of the studies are summarized in 2 separate mini meta-analyses. Participants implicitly associated young adult individuals with positive emotions, that is, happiness and serenity, respectively, and old adult individuals with negative emotions, that is, sadness and anger, respectively (Mini Meta-Analysis 1). Within negative emotions, participants preferentially associated young adult individuals with sadness and old adult individuals with anger (Mini Meta-Analysis 2). Even though young and old adults are stereotypically associated with specific emotions, contextual factors influence which age-emotion stereotype is salient in a given context.},
	language = {en},
	urldate = {2019-12-03},
	journal = {Emotion},
	author = {Freudenberg, Maxi and Albohn, Daniel N. and Kleck, Robert E. and Adams, Reginald B. and Hess, Ursula},
	month = jul,
	year = {2019},
	file = {Freudenberg et al. - 2019 - Emotional stereotypes on trial Implicit emotion a.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/JG67SKUC/Freudenberg et al. - 2019 - Emotional stereotypes on trial Implicit emotion a.pdf:application/pdf},
}

@incollection{albohn2019PerceivingEmotionNeutral,
	address = {Cham},
	title = {Perceiving {Emotion} in the “{Neutral}” {Face}: {A} {Powerful} {Mechanism} of {Person} {Perception}},
	isbn = {978-3-030-32967-9 978-3-030-32968-6},
	shorttitle = {Perceiving {Emotion} in the “{Neutral}” {Face}},
	url = {http://link.springer.com/10.1007/978-3-030-32968-6_3},
	language = {en},
	urldate = {2019-12-17},
	booktitle = {The {Social} {Nature} of {Emotion} {Expression}},
	publisher = {Springer International Publishing},
	author = {Albohn, Daniel N. and Brandenburg, Joseph C. and Adams, Reginald B.},
	editor = {Hess, Ursula and Hareli, Shlomo},
	year = {2019},
	doi = {10.1007/978-3-030-32968-6_3},
	pages = {25--47},
	file = {Albohn et al. - 2019 - Perceiving Emotion in the “Neutral” Face A Powerf.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/TAPHTDAM/Albohn et al. - 2019 - Perceiving Emotion in the “Neutral” Face A Powerf.pdf:application/pdf},
}

@article{kveragaSpatialFeaturebasedAttention2019,
	title = {Spatial and feature-based attention to expressive faces},
	volume = {237},
	issn = {0014-4819, 1432-1106},
	url = {http://link.springer.com/10.1007/s00221-019-05472-8},
	doi = {10.1007/s00221-019-05472-8},
	abstract = {Facial emotion is an important cue for deciding whether an individual is potentially helpful or harmful. However, facial expressions are inherently ambiguous and observers typically employ other cues to categorize emotion expressed on the face, such as race, sex, and context. Here, we explored the effect of increasing or reducing different types of uncertainty associated with a facial expression that is to be categorized. On each trial, observers responded according to the emotion and location of a peripherally presented face stimulus and were provided with either: (1) no information about the upcoming face; (2) its location; (3) its expressed emotion; or (4) both its location and emotion. While cueing emotion or location resulted in faster response times than cueing unpredictive information, cueing face emotion alone resulted in faster responses than cueing face location alone. Moreover, cueing both stimulus location and emotion resulted in a superadditive reduction of response times compared with cueing location or emotion alone, suggesting that feature-based attention to emotion and spatially selective attention interact to facilitate perception of face stimuli. While categorization of facial expressions was significantly affected by stable identity cues (sex and race) in the face, we found that these interactions were eliminated when uncertainty about facial expression, but not spatial uncertainty about stimulus location, was reduced by predictive cueing. This demonstrates that feature-based attention to facial expression greatly attenuates the need to rely on stable identity cues to interpret facial emotion.},
	language = {en},
	number = {4},
	urldate = {2020-03-05},
	journal = {Experimental Brain Research},
	author = {Kveraga, Kestutis and De Vito, David and Cushing, Cody and Im, Hee Yeon and Albohn, Daniel N. and Adams, Reginald B.},
	month = apr,
	year = {2019},
	pages = {967--975},
	file = {Kveraga et al. - 2019 - Spatial and feature-based attention to expressive .pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/F9DF9GZY/Kveraga et al. - 2019 - Spatial and feature-based attention to expressive .pdf:application/pdf},
}

@article{albohn2020EverydayBeliefsEmotion,
	title = {Everyday {Beliefs} {About} {Emotion} {Perceptually} {Derived} {From} {Neutral} {Facial} {Appearance}},
	volume = {11},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2020.00264/full},
	doi = {10.3389/fpsyg.2020.00264},
	abstract = {The evolution of the human brain and visual system is widely believed to have been shaped by the need to process and make sense out of expressive information, particularly via the face. We are so attuned to expressive information in the face that it informs even stable trait inferences (e.g., Knutson, 1996) through a process we refer to here as the face-speciﬁc fundamental attribution error (Albohn et al., 2019). We even derive highly consistent beliefs about the emotional lives of others based on emotion-resembling facial appearance (e.g., low versus high brows, big versus small eyes, etc.) in faces we know are completely devoid of overt expression (i.e., emotion overgeneralization effect: see Zebrowitz et al., 2010). The present studies extend these insights to better understand lay beliefs about older and younger adults’ emotion dispositions and their impact on behavioral outcomes. In Study 1, we found that older versus younger faces objectively have more negative emotion-resembling cues in the face (using computer vision), and that raters likewise attribute more negative emotional dispositions to older versus younger adults based just on neutral facial appearance (see too Adams et al., 2016). In Study 2, we found that people appear to encode these negative emotional appearance cues in memory more so for older than younger adult faces. Finally, in Study 3 we exam downstream behavioral consequences of these negative attributions, showing that observers’ avoidance of older versus younger faces is mediated by emotion-resembling facial appearance.},
	language = {en},
	urldate = {2020-03-17},
	journal = {Frontiers in Psychology},
	author = {Albohn, Daniel N. and Adams, Reginald B.},
	month = feb,
	year = {2020},
	pages = {264},
	file = {Albohn and Adams - 2020 - Everyday Beliefs About Emotion Perceptually Derive.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/3ZY3AA67/Albohn and Adams - 2020 - Everyday Beliefs About Emotion Perceptually Derive.pdf:application/pdf;Albohn and Adams - 2020 - Everyday Beliefs About Emotion Perceptually Derive.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/VFMJFBHI/Albohn and Adams - 2020 - Everyday Beliefs About Emotion Perceptually Derive.pdf:application/pdf},
}

@article{albohn2020EmotionResidueNeutral,
	title = {Emotion {Residue} in {Neutral} {Faces}: {Implications} for {Impression} {Formation}},
	issn = {1948-5506, 1948-5514},
	shorttitle = {Emotion {Residue} in {Neutral} {Faces}},
	url = {http://journals.sagepub.com/doi/10.1177/1948550620923229},
	doi = {10.1177/1948550620923229},
	abstract = {Despite the prevalent use of neutral faces in expression research, the term neutral still remains ill-defined and understudied. A general assumption is that one’s overt attempt to pose a nonexpressive face results in a neutral display, one devoid of any expressive information. Ample research has demonstrated that nonexpressive faces do convey meaning, however, through emotion-resembling appearance. Here, we examined whether prior expressive information lingers on a face, in the form of emotion residue, and whether despite overt attempts to display a neutral face, these subtle emotion cues influence trait impressions. Across three studies, we found that explicit attempts at posing neutral displays retained emotion residue from a prior expression. This residue in turn significantly impacted the impressions formed of these otherwise “neutral” displays. We discuss implications of this work for better understanding how accurate impressions are derived from the so-called neutral faces and underscore theoretical and methodological considerations for future research.},
	language = {en},
	urldate = {2020-09-14},
	journal = {Social Psychological and Personality Science},
	author = {Albohn, Daniel N. and Adams, Reginald B.},
	month = jun,
	year = {2020},
	pages = {194855062092322},
	file = {Albohn and Adams - 2020 - Emotion Residue in Neutral Faces Implications for.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/CH3HU9I8/Albohn and Adams - 2020 - Emotion Residue in Neutral Faces Implications for.pdf:application/pdf},
}

@incollection{adamsDifferentialMagnocellularParvocellular2019,
	title = {Differential magnocellular versus parvocellular pathway contributions to the combinatorial processing of facial threat},
	volume = {247},
	isbn = {978-0-444-64252-3},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0079612319300366},
	abstract = {Recently, speed of presentation of facially expressive stimuli was found to influence the processing of compound threat cues (e.g., anger/fear/gaze). For instance, greater amygdala responses were found to clear (e.g., direct gaze anger/averted gaze fear) versus ambiguous (averted gaze anger/direct gaze fear) combinations of threat cues when rapidly presented (33 and 300 ms), but greater to ambiguous versus clear threat cues when presented for more sustained durations (1, 1.5, and 2 s). A working hypothesis was put forth (Adams et al., 2012) that these effects were due to differential magnocellular versus parvocellular pathways contributions to the rapid versus sustained processing of threat, respectively. To test this possibility directly here, we restricted visual stream processing in the fMRI environment using facially expressive stimuli specifically designed to bias visual input exclusively to the magnocellular versus parvocellular pathways. We found that for magnocellular-biased stimuli, activations were predominantly greater to clear versus ambiguous threat-gaze pairs (on par with that previously found for rapid presentations of threat cues), whereas activations to ambiguous versus clear threat-gaze pairs were greater for parvocellular-biased stimuli (on par with that previously found for sustained presentations). We couch these findings in an adaptive dual process account of threat perception and highlight implications for other dual process models within psychology.},
	language = {en},
	urldate = {2020-09-24},
	booktitle = {Progress in {Brain} {Research}},
	publisher = {Elsevier},
	author = {Adams, Reginald B. and Im, Hee Yeon and Cushing, Cody and Boshyan, Jasmine and Ward, Noreen and Albohn, Daniel N. and Kveraga, Kestutis},
	year = {2019},
	doi = {10.1016/bs.pbr.2019.03.006},
	pages = {71--87},
	file = {Adams et al. - 2019 - Differential magnocellular versus parvocellular pa.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/85Z99D8A/Adams et al. - 2019 - Differential magnocellular versus parvocellular pa.pdf:application/pdf},
}

@article{choCultureModeratesRelationship2018,
	title = {Culture {Moderates} the {Relationship} {Between} {Emotional} {Fit} and {Collective} {Aspects} of {Well}-{Being}},
	volume = {9},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2018.01509/full},
	doi = {10.3389/fpsyg.2018.01509},
	language = {en},
	urldate = {2020-09-24},
	journal = {Frontiers in Psychology},
	author = {Cho, Sinhae and Doren, Natalia Van and Minnick, Mark R. and Albohn, Daniel N. and Adams, Reginald B. and Soto, José A.},
	month = aug,
	year = {2018},
	pages = {1509},
	file = {Cho et al. - 2018 - Culture Moderates the Relationship Between Emotion.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/XKG2PKHG/Cho et al. - 2018 - Culture Moderates the Relationship Between Emotion.pdf:application/pdf;Cho et al. - 2018 - Culture Moderates the Relationship Between Emotion.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/LGWKJ79E/Cho et al. - 2018 - Culture Moderates the Relationship Between Emotion.pdf:application/pdf},
}

@article{imCrossculturalHemisphericLaterality2017,
	title = {Cross-cultural and hemispheric laterality effects on the ensemble coding of emotion in facial crowds},
	volume = {5},
	issn = {2193-8652, 2193-8660},
	url = {http://link.springer.com/10.1007/s40167-017-0054-y},
	doi = {10.1007/s40167-017-0054-y},
	language = {en},
	number = {2},
	urldate = {2020-09-24},
	journal = {Culture and Brain},
	author = {Im, Hee Yeon and Chong, Sang Chul and Sun, Jisoo and Steiner, Troy G. and Albohn, Daniel N. and Adams, Reginald B. and Kveraga, Kestutis},
	month = oct,
	year = {2017},
	pages = {125--152},
	file = {Im et al. - 2017 - Cross-cultural and hemispheric laterality effects .pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/G9IK2V25/Im et al. - 2017 - Cross-cultural and hemispheric laterality effects .pdf:application/pdf},
}

@article{rush2017EffectsMindfulnessBiofeedback,
	title = {The {Effects} of a {Mindfulness} and {Biofeedback} {Program} on the {On}- and {Off}-{Task} {Behaviors} of {Students} with {Emotional} {Behavioral} {Disorders}},
	volume = {21},
	issn = {2159-2020, 2161-1505},
	url = {http://link.springer.com/10.1007/s40688-017-0140-3},
	doi = {10.1007/s40688-017-0140-3},
	language = {en},
	number = {4},
	urldate = {2020-09-24},
	journal = {Contemporary School Psychology},
	author = {Rush, Karena S. and Golden, Maria E. and Mortenson, Bruce P. and Albohn, Daniel N. and Horger, Melissa},
	month = dec,
	year = {2017},
	pages = {347--357},
	file = {Rush et al. - 2017 - The Effects of a Mindfulness and Biofeedback Progr.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/T76GDWK4/Rush et al. - 2017 - The Effects of a Mindfulness and Biofeedback Progr.pdf:application/pdf},
}

@incollection{adams2017SocialVisionAccount,
	edition = {2},
	title = {A {Social} {Vision} {Account} of {Facial} {Expression} {Perception}},
	url = {https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780190613501.001.0001/acprof-9780190613501},
	booktitle = {The {Science} of {Facial} {Expressions}},
	publisher = {Oxford University Press},
	author = {Adams, Reginal B. and Albohn, Daniel N. and Kveraga, Kestutis},
	editor = {Fernandez-Dols, Jose-Miguel and Russell, James A},
	year = {2017},
	pages = {315--332},
}

@article{albohn2021ExpressiveTriadStructure,
	title = {The {Expressive} {Triad}: {Structure}, {Color}, and {Texture} {Similarity} of {Emotion} {Expressions} {Predict} {Impressions} of {Neutral} {Faces}},
	volume = {12},
	issn = {1664-1078},
	shorttitle = {The {Expressive} {Triad}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.612923/abstract},
	doi = {10.3389/fpsyg.2021.612923},
	abstract = {Previous research has demonstrated how emotion resembling cues in the face help shape impression formation (i.e., emotion overgeneralization). Perhaps most notable in the literature to date, has been work suggesting that gender-related appearance cues are visually confounded with certain stereotypic expressive cues (see Adams, Hess, \& Kleck, 2015 for review). Only a couple studies to date have used computer vision to directly map out and test facial structural resemblance to emotion expressions using facial landmark coordinates to estimate face shape. In one study using a Bayesian network classifier trained to detect emotional expressions structural resemblance to a specific expression on a non-expressive (i.e., neutral) face was found to influence trait impressions of others (Said et al., 2009). In another study, a connectionist model trained to detect emotional expressions found different emotion-resembling cues in male versus female faces (Zebrowitz et al., 2010). Despite this seminal work, direct evidence confirming the theoretical assertion that humans likewise utilize these emotion-resembling cues when forming impressions has been lacking. Across four studies, we replicate and extend these prior findings using new advances in computer vision to examine gender-related, emotion-resembling structure, color, and texture (as well as their weighted combination) and their impact on gender-stereotypic impression formation. We show that all three (plus their combination) are meaningfully related to human impressions of emotionally neutral faces. Further when applying the computer vision algorithms to experimentally manipulate faces, we show that humans derive similar impressions from them as did the computer.},
	language = {English},
	urldate = {2021-01-21},
	journal = {Frontiers in Psychology},
	author = {Albohn, Daniel N. and Adams, Reginal B.},
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {machine learning, face perception, Emotion Expression, facial expresions, impression formation},
	file = {Albohn and Jr - 2021 - The Expressive Triad Structure, Color, and Textur.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/ULGSVA2J/Albohn and Jr - 2021 - The Expressive Triad Structure, Color, and Textur.pdf:application/pdf},
}

@article{brandenburgFacingSocialExclusion2022,
	title = {Facing social exclusion: a facial {EMG} examination of the reaffiliative function of smiling},
	issn = {0269-9931, 1464-0600},
	shorttitle = {Facing social exclusion},
	url = {https://www.tandfonline.com/doi/full/10.1080/02699931.2022.2041404},
	doi = {10.1080/02699931.2022.2041404},
	abstract = {Social exclusion inﬂuences how expressions are perceived and the tendency of the perceiver to mimic them. However, less is known about social exclusion’s eﬀect on one’s own facial expressions. The aim of the present study was to identify the eﬀects of social exclusion on Duchenne smiling behaviour, deﬁned as activity of both zygomaticus major and the orbicularis oculi muscles. Utilising a withinsubject’s design, participants took part in the Cyberball Task in which they were both included and excluded while facial electromyography was measured. We found that during the active experience of social exclusion, participants showed greater orbicularis oculi activation when compared to the social inclusion condition. Further, we found that across both conditions, participants showed greater zygomaticus major muscle activation the longer they engaged in the Cyberball Task. Order of condition also mattered, with those who experienced social exclusion before social inclusion showing the greatest overall muscle activation. These results are consistent with an aﬃliative function of smiling, particularly as social exclusion engaged activation of muscles associated with a Duchenne smile.},
	language = {en},
	urldate = {2022-02-18},
	journal = {Cognition and Emotion},
	author = {Brandenburg, Joseph C. and Albohn, Daniel N. and Bernstein, Michael J. and Soto, Jose A. and Hess, Ursula and Adams, Reginald B.},
	month = feb,
	year = {2022},
	pages = {1--9},
	file = {Brandenburg et al. - 2022 - Facing social exclusion a facial EMG examination .pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/AUG24TDB/Brandenburg et al. - 2022 - Facing social exclusion a facial EMG examination .pdf:application/pdf},
}

@article{adams2022AngryWhiteFaces,
	title = {Angry {White} {Faces}: {A} {Contradiction} of {Racial} {Stereotypes} and {Emotion}-{Resembling} {Appearance}},
	issn = {2662-2041, 2662-205X},
	shorttitle = {Angry {White} {Faces}},
	url = {https://link.springer.com/10.1007/s42761-021-00091-5},
	doi = {10.1007/s42761-021-00091-5},
	abstract = {Machine learning findings suggest Eurocentric (aka White/European) faces structurally resemble anger more than Afrocentric (aka Black/African) faces (e.g., Albohn, 2020; Zebrowitz et al., 2010); however, Afrocentric faces are typically associated with anger more so than Eurocentric faces (e.g., Hugenberg \& Bodenhausen, 2003, 2004). Here, we further examine counter-stereotypic associations between Eurocentric faces and anger, and Afrocentric faces and fear. In Study 1, using a computer vision algorithm, we demonstrate that neutral European American faces structurally resemble anger more and fear less than do African American faces. In Study 2, we then found that anger- and fear-resembling facial appearance influences perceived racial prototypicality in this same counter-stereotypic manner. In Study 3, we likewise found that imagined European American versus African American faces were rated counter-stereotypically (i.e., more like anger than fear) on key emotion-related facial characteristics (i.e., size of eyes, size of mouth, overall angularity of features). Finally in Study 4, we again found counter-stereotypic differences, this time in processing fluency, such that angry Eurocentric versus Afrocentric faces and fearful Afrocentric versus Eurocentric faces were categorized more accurately and quickly. Only in Study 5, using race-ambiguous interior facial cues coupled with Afrocentric versus Eurocentric hairstyles and skin tone, did we find the stereotypical effects commonly reported in the literature. These findings are consistent with the conclusion that the “angry Black” association in face perception is socially constructed in that structural cues considered prototypical of African American appearance conflict with common race-emotion stereotypes.},
	language = {en},
	urldate = {2022-03-01},
	journal = {Affective Science},
	author = {Adams, Reginald B. and Albohn, Daniel N. and Hedgecoth, Nicole and Garrido, Carlos O. and Adams, Katharine Donnelly},
	month = mar,
	year = {2022},
	file = {Adams et al. - 2022 - Angry White Faces A Contradiction of Racial Stere.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/FX4BS5E7/Adams et al. - 2022 - Angry White Faces A Contradiction of Racial Stere.pdf:application/pdf},
}

@article{albohnSocialFaceHypothesis2022,
	title = {The {Social} {Face} {Hypothesis}},
	issn = {2662-2041, 2662-205X},
	url = {https://link.springer.com/10.1007/s42761-022-00116-7},
	doi = {10.1007/s42761-022-00116-7},
	abstract = {Meeting the demands of a social world is an incredibly complex task. Since humans are able to navigate the social world so effortlessly, our ability to both interpret and signal complex social and emotional information is arguably shaped by evolutionary pressures. Dunbar (1992) tested this assumption in his Social Brain Hypothesis, observing that different primates’ neocortical volume predicted their average social network size, suggesting that neocortical evolution was driven at least in part by social demands. Here we examined the Social Face Hypothesis, based on the assumption that the face co-evolved with the brain to signal more complex and nuanced emotional, mental, and behavioral states to others. Despite prior observations suggestive of this conclusion (e.g., Redican, 1982), it has not, to our knowledge, been empirically tested. To do this, we obtained updated metrics of primate facial musculature, facial hair bareness, average social network size, and average brain weight data for a large number of primate genera (N = 63). In this sample, we replicated Dunbar’s original observation by finding that average brain weight predicted average social network size. Critically, we also found that perceived facial hair bareness predicted both group size and average brain weight. Finally, we found that all three variables acted as mediators, confirming a complex, interdependent relationship between primate social network size, primate brain weight, and primate facial hair bareness. These findings are consistent with the conclusion that the primate brain and face co-evolved in response to meeting the increased social demands of one’s environment.},
	language = {en},
	urldate = {2022-05-23},
	journal = {Affective Science},
	author = {Albohn, Daniel N. and Adams, Reginald B.},
	month = may,
	year = {2022},
	file = {Albohn and Adams - 2022 - The Social Face Hypothesis.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/XWC83L7L/Albohn and Adams - 2022 - The Social Face Hypothesis.pdf:application/pdf},
}

@article{albohnSharedSignalHypothesis2022,
	title = {The shared signal hypothesis: {Facial} and bodily expressions of emotion mutually inform one another},
	issn = {1943-3921, 1943-393X},
	shorttitle = {The shared signal hypothesis},
	url = {https://link.springer.com/10.3758/s13414-022-02548-6},
	doi = {10.3758/s13414-022-02548-6},
	abstract = {Decades of research show that contextual information from the body, visual scene, and voices can facilitate judgments of facial expressions of emotion. To date, most research suggests that bodily expressions of emotion offer context for interpreting facial expressions, but not vice versa. The present research aimed to investigate the conditions under which mutual processing of facial and bodily displays of emotion facilitate and/or interfere with emotion recognition. In the current two studies, we examined whether body and face emotion recognition are enhanced through integration of shared emotion cues, and/or hindered through mixed signals (i.e., interference). We tested whether faces and bodies facilitate or interfere with emotion processing by pairing briefly presented (33 ms), backward-masked presentations of faces with supraliminally presented bodies (Experiment 1) and vice versa (Experiment 2). Both studies revealed strong support for integration effects, but not interference. Integration effects are most pronounced for low-emotional clarity facial and bodily expressions, suggesting that when more information is needed in one channel, the other channel is recruited to disentangle any ambiguity. That this occurs for briefly presented, backward-masked presentations reveals low-level visual integration of shared emotional signal value.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Albohn, Daniel N. and Brandenburg, Joseph C. and Kveraga, Kestutis and Adams, Reginald B.},
	month = aug,
	year = {2022},
	file = {Albohn et al. - 2022 - The shared signal hypothesis Facial and bodily ex.pdf:/Users/dalbohn/Documents/Reference/Zotero/storage/QICSULTW/Albohn et al. - 2022 - The shared signal hypothesis Facial and bodily ex.pdf:application/pdf},
}

@article{albohnDatadrivenHyperrealisticMethod2022,
	title = {A data-driven, hyper-realistic method for visualizing individual mental representations of faces},
	volume = {13},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2022.997498},
	abstract = {Research in person and face perception has broadly focused on group-level consensus that individuals hold when making judgments of others (e.g., “X type of face looks trustworthy”). However, a growing body of research demonstrates that individual variation is larger than shared, stimulus-level variation for many social trait judgments. Despite this insight, little research to date has focused on building and explaining individual models of face perception. Studies and methodologies that have examined individual models are limited in what visualizations they can reliably produce to either noisy and blurry or computer avatar representations. Methods that produce low-fidelity visual representations inhibit generalizability by being clearly computer manipulated and produced. In the present work, we introduce a novel paradigm to visualize individual models of face judgments by leveraging state-of-the-art computer vision methods. Our proposed method can produce a set of photorealistic face images that correspond to an individual's mental representation of a specific attribute across a variety of attribute intensities. We provide a proof-of-concept study which examines perceived trustworthiness/untrustworthiness and masculinity/femininity. We close with a discussion of future work to substantiate our proposed method.},
	urldate = {2022-09-28},
	journal = {Frontiers in Psychology},
	author = {Albohn, Daniel N. and Uddenberg, Stefan and Todorov, Alexander},
	year = {2022},
	file = {Full Text PDF:/Users/dalbohn/Documents/Reference/Zotero/storage/FQS57ILF/Albohn et al. - 2022 - A data-driven, hyper-realistic method for visualiz.pdf:application/pdf},
}

@article{todorov2022GenerativeModelsVisualizing,
	title = {Generative models for visualizing idiosyncratic impressions},
	issn = {0007-1269, 2044-8295},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjop.12622},
	doi = {10.1111/bjop.12622},
	language = {en},
	urldate = {2023-01-31},
	journal = {British Journal of Psychology},
	author = {Todorov, Alexander and Uddenberg, Stefan and Albohn, Daniel N.},
	month = dec,
	year = {2022},
	keywords = {data-driven methods, faces, idiosyncratic differences, impressions},
	pages = {bjop.12622},
	file = {Full Text PDF:/Users/dalbohn/Documents/Reference/Zotero/storage/ZNH6KKLD/Todorov et al. - Generative models for visualizing idiosyncratic im.pdf:application/pdf;Snapshot:/Users/dalbohn/Documents/Reference/Zotero/storage/GSHGVPKD/bjop.html:text/html},
}
